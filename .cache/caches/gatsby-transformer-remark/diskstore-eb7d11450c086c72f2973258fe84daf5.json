{"expireTime":9007200809613581000,"key":"transformer-remark-markdown-html-610f73a6de44f7b1663e3ea0f3d44c58-gatsby-remark-autolink-headersgatsby-remark-prismjsgatsby-remark-external-links-","val":"<blockquote>\n<p><a href=\"https://kaggle.com\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kaggle</a> is an online community of data scientists and machine learners, owned by Google LLC. Users can find datasets and participate in competitions for prizes and more importantly, for experience!</p>\n</blockquote>\n<p>Check out my <a href=\"https://kaggle.com/davidcgong\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Kaggle profile</a>!</p>\n<p><img src=\"data:image/png;base64,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\" alt=\"Kaggle\"></p>\n<h2 id=\"what-is-kaggle\"><a href=\"#what-is-kaggle\" aria-label=\"what is kaggle permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What is Kaggle?</h2>\n<p>While attempting to gain a better understanding about the world of data science, I came around one platform that served as an extremely insightful tool named Kaggle. On Kaggle, one can find datasets, participate in competitions, and connect with other data scientists and share knowledge.</p>\n<p>I chose to delve into learning about Kaggle competitions to learn about what worked, what doesn't, and how it would be applicable in real life situations. Surprisingly, I was able to place within the top 15% of competition for some competitions (Microsoft Malware Prediction and Elo Merchant Category Recommendation) even as a beginner.</p>\n<h2 id=\"kaggle-competitions\"><a href=\"#kaggle-competitions\" aria-label=\"kaggle competitions permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kaggle competitions?</h2>\n<p>To simply put it, anyone can participate in a Kaggle competition, in which you are given an overview of the problem along with datasets. It is then entirely up to you to determine how to make use of these datasets and determine the problem in your way. </p>\n<p>Of course, often times you will find many kernels and discussions that will help you start out with a problem.</p>\n<p>When submitting predictions prior to the competition deadline, you will receive an initial score based on the performance of your predictions with the current set of training data. </p>\n<p>The final score you obtain may be used against future data, against the rest of the training data, and such.</p>\n<h2 id=\"producing-accurate-results-in-general\"><a href=\"#producing-accurate-results-in-general\" aria-label=\"producing accurate results in general permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Producing accurate results in general</h2>\n<p><img src=\"https://elitedatascience.com/wp-content/uploads/2018/05/What-Goes-Into-a-Successful-Model.jpg\" alt=\"A guideline for generating a successful model\"></p>\n<p>Algorithm selection in my opinion should take a little bit more precedence than 'other', but the key takeaway is that this diagram just shows that having one flaw when producing a model can generate substandard and undesired results.</p>\n<h2 id=\"three-things-to-think-about\"><a href=\"#three-things-to-think-about\" aria-label=\"three things to think about permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Three things to think about</h2>\n<p>When it comes to deciding the approach to doing well on Kaggle competitions, there are probably three basic, fundamental ideas to consider outside of EDA (exploratory data analysis).</p>\n<h3 id=\"1-algorithm-selection-boosting-neural-networks\"><a href=\"#1-algorithm-selection-boosting-neural-networks\" aria-label=\"1 algorithm selection boosting neural networks permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1) Algorithm selection (Boosting, neural networks?)</h3>\n<p>For most competitions and datasets that involve some form of supervised learning, typically the best approach is to use gradient boosting. <strong>In particular, Kagglers are heavily biased towards using <a href=\"https://xgboost.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">XGBoost</a> and <a href=\"https://lightgbm.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">LightGBM</a></strong>. </p>\n<p>Other ideas such as neural networks may work as well. Standalone decision trees on the other hand, well maybe not so much when it comes to accuracy. Kaggle doesn't care about performance, but only cares about accuracy.</p>\n<p><strong>Blending is also a popular technique used by many Kagglers</strong>, which essentially combines many test results using various algorithms (and hyperparameters) through calculating the mean or using a specified weight for determining values.</p>\n<p>That said, there are also various other problems that require different solutions. For example, you might consider a convolutional neural network for an image classification problem as the previous algorithms either don't work as well or are just incompatible.</p>\n<h3 id=\"2-feature-engineering\"><a href=\"#2-feature-engineering\" aria-label=\"2 feature engineering permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2) Feature engineering</h3>\n<blockquote>\n<p>Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. -Wikipedia</p>\n</blockquote>\n<p>A lot of times you are thrown a bunch of features and categories that you have to determine that are worth to be relevant or not. Many times, the training dataset that is given to you often only has a few features. Running an algorithm on that dataset probably wouldn't get you very far in terms of accuracy.</p>\n<p><strong>Learning to rank features by importance and merging them with the training dataset is ultimately what will separate the Kaggle competition expert from the beginner.</strong> Afterwards, it comes down to understanding the problem and adjusting hyperparameters. </p>\n<h3 id=\"3-cloud-computing\"><a href=\"#3-cloud-computing\" aria-label=\"3 cloud computing permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3) Cloud computing</h3>\n<p>Depending on the size of the dataset, the competition problem, and the way you constructed your model (feature engineering), your model may take anywhere from 30 minutes to a day (maybe even days) to train and generate predictions. </p>\n<p>In many cases, running models on the Kaggle kernel is enough to solve this problem. Your model trains offline on Kaggle's server, which is perfect for the busy student who is just tuning hyperparameters on a model that takes more than 20 hours to run! <em>Not talking about me by the way...</em></p>\n<p>A few other options would be to employ AWS instances to quickly deploy a machine learning model, which can often be faster and less cost-expensive than utilizing your own GPU and RAM.</p>\n<p>Overall, however, I would recommend just sticking with the kernel and learning about the various approaches to competition problems and feature engineering as mentioned above.</p>\n<h2 id=\"closing-thoughts\"><a href=\"#closing-thoughts\" aria-label=\"closing thoughts permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Closing thoughts</h2>\n<p>These are once again very basic ideas that one should know before going into a competition, but I believe that developing good fundamentals over time is much more important. Read books, look through notebooks of past winners, adopt their methods, and then you might just get that Kaggle gold medal one day. :)</p>"}